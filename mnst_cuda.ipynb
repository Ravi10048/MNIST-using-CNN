{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwVdQzIN2Kta6Pc5bDkBpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravi10048/MNIST-using-CNN/blob/main/mnst_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bppV5hLRB7uI",
        "outputId": "465e8ed0-33c4-4f1e-c9c9-a3d1a63ab7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.cuda\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Torchvision module contains various utilities, classes, models and datasets \n",
        "# used towards computer vision usecases\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "# Functional module contains helper functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''Create the Dataloder'''\n",
        "# Mean and standard deviation of all the pixels in the MNIST dataset\n",
        "mean_gray = 0.1307\n",
        "stddev_gray = 0.3081\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((mean_gray,), (stddev_gray,))]) # grayscale has 1 mean_gray wheras color has 3\n",
        "\n",
        "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "'''whenever the new image will get download it will first convert to tensor then normalize'''\n",
        "\n",
        "img = mnist_train[12][0].numpy() * stddev_gray + mean_gray\n",
        "plt.imshow(img.reshape(28, 28), cmap='gray') #converting tensor to numpy\n",
        "# plt.show()\n",
        "# Note that each image is 28 x 28 pixe\n",
        "\n",
        "label = mnist_train[12][1]\n",
        "print(label) #3\n",
        "batch_size = 1024 # Reduce this if you get out-of-memory error\n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "class MNISTNet(nn.Module): #The CNN model is defined in the MNISTNet class, which consists of two convolutional layers and two fully connected layers.\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "               \n",
        "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
        "        # which is what we are using.\n",
        "        \n",
        "        # Convolution Layer 1                             # 28 x 28 x 1  (input)\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
        "        self.relu1 = nn.ReLU()                            # Same as above\n",
        "        \n",
        "        # Convolution Layer 2\n",
        "        self.conv2 = nn.Conv2d(20, 30, kernel_size=5)     # 20 x 20 x 30  (after 2nd convolution)\n",
        "        self.conv2_drop = nn.Dropout2d(p=0.5)             # Same as above\n",
        "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 30  (after pooling)\n",
        "        self.relu2 = nn.ReLU()                            # Same as above \n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(3000, 500)  # 10 *10*30 output of above and input for this\n",
        "        self.fc2 = nn.Linear(500, 10)  # specify no. of class label\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Convolution Layer 1                    \n",
        "        x = self.conv1(x)                        \n",
        "        x = self.relu1(x)                        \n",
        "        \n",
        "        # Convolution Layer 2\n",
        "        x = self.conv2(x)               \n",
        "        x = self.conv2_drop(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.relu2(x)\n",
        "        \n",
        "        # Switch from activation maps to vectors\n",
        "        x = x.view(-1, 3000)\n",
        "        \n",
        "        # Fully connected layer 1\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=True)\n",
        "        \n",
        "        # Fully connected layer 2\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "# Create the objects\n",
        "if __name__ == '__main__':\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    # The model\n",
        "    net = MNISTNet()\n",
        "\n",
        "    if cuda.is_available():\n",
        "        net = net.cuda()\n",
        "\n",
        "    # Our loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Our optimizer\n",
        "    learning_rate = 0.01\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) \n",
        "\n",
        "    '''Training loop'''\n",
        "\n",
        "    num_epochs = 100 #100\n",
        "\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        ############################\n",
        "        # Train\n",
        "        ############################\n",
        "        iter_loss = 0.0\n",
        "        correct = 0\n",
        "        iterations = 0\n",
        "        \n",
        "        net.train()                   # Put the network into training mode by each iteration\n",
        "        \n",
        "        for i, (items, classes) in enumerate(mnist_train_loader):\n",
        "            \n",
        "            # Convert torch tensor to Variable\n",
        "            items = Variable(items)\n",
        "            classes = Variable(classes)\n",
        "            \n",
        "            # If we have GPU, shift the data to GPU\n",
        "            if cuda.is_available():\n",
        "                items = items.cuda()\n",
        "                classes = classes.cuda()\n",
        "            \n",
        "            optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
        "            outputs = net(items)      # Do the forward pass\n",
        "            loss = criterion(outputs, classes) # Calculate the loss\n",
        "            iter_loss += loss.item() # Accumulate the loss\n",
        "            loss.backward()           # Calculate the gradients with help of back propagation\n",
        "            optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
        "            \n",
        "            # Record the correct predictions for training data \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            if torch.cuda.is_available():\n",
        "                 correct += (predicted.cpu() == classes.data.cpu()).sum().item()\n",
        "            else:\n",
        "                correct += (predicted == classes.data).sum().item()\n",
        "            # correct += (predicted == classes.data).sum()\n",
        "            iterations += 1\n",
        "        \n",
        "        # Record the training loss\n",
        "        train_loss.append(iter_loss/iterations)\n",
        "        # Record the training accuracy\n",
        "        train_accuracy.append((100 * correct / len(mnist_train_loader.dataset)))\n",
        "    \n",
        "\n",
        "        ############################\n",
        "        # Validate - How did we do on the unseen dataset?\n",
        "        ############################\n",
        "        \n",
        "        loss = 0.0\n",
        "        correct = 0\n",
        "        iterations = 0\n",
        "\n",
        "        net.eval()                    # Put the network into evaluate mode optimizer stop\n",
        "        \n",
        "        for i, (items, classes) in enumerate(mnist_valid_loader):\n",
        "            \n",
        "            # Convert torch tensor to Variable\n",
        "            items = Variable(items)\n",
        "            classes = Variable(classes)\n",
        "            \n",
        "            # If we have GPU, shift the data to GPU\n",
        "            if cuda.is_available():\n",
        "                items = items.cuda()\n",
        "                classes = classes.cuda()\n",
        "            \n",
        "            outputs = net(items)      # Do the forward pass\n",
        "            loss += criterion(outputs, classes).item() # Calculate the loss\n",
        "            \n",
        "            # Record the correct predictions for training data\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            if torch.cuda.is_available():\n",
        "                 correct += (predicted.cpu() == classes.data.cpu()).sum().item()\n",
        "            else:\n",
        "                correct += (predicted == classes.data).sum().item()\n",
        "            # correct += (predicted == classes.data).sum()\n",
        "            \n",
        "            iterations += 1\n",
        "\n",
        "        # Record the validation loss\n",
        "        valid_loss.append(loss/iterations)\n",
        "        # Record the validation accuracy\n",
        "        valid_accuracy.append(correct / len(mnist_valid_loader.dataset) * 100.0)\n",
        "\n",
        "        \n",
        "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
        "            %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
        "                valid_loss[-1], valid_accuracy[-1]))\n",
        "        \n",
        "    # Loss\n",
        "   \n",
        "    f = plt.figure(figsize=(10, 8))\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(valid_loss, label='validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Accuracy\n",
        "    # # if using cuda\n",
        "   \n",
        "    import numpy as np\n",
        "    f = plt.figure(figsize=(10, 8))\n",
        "    # # if using cuda\n",
        "    plt.plot(np.array(train_accuracy), label='training accuracy')\n",
        "    plt.plot(np.array(valid_accuracy), label='validation accuracy')\n",
        "    # else\n",
        "    # plt.plot(train_accuracy, label='training accuracy')\n",
        "    # plt.plot(valid_accuracy, label='validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    torch.save(net.state_dict(), \"./mnist.model.pth\")\n",
        "net=MNISTNet()\n",
        "net.load_state_dict(torch.load(\"./mnist.model.pth\"))\n",
        "        \n",
        "'''Standalone inference'''\n",
        "\n",
        "image_index = 23\n",
        "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
        "img = Variable(img)\n",
        "label = mnist_valid[image_index][1]\n",
        "\n",
        "net.eval()\n",
        "\n",
        "if cuda.is_available():\n",
        "    net = net.cuda()\n",
        "    img = img.cuda()\n",
        "else:\n",
        "    net = net.cpu()\n",
        "    img = img.cpu()\n",
        "    \n",
        "output = net(img)\n",
        "\n",
        "print(output.data)\n",
        "\n",
        "_, predicted = torch.max(output.data, 1)\n",
        "print(\"Prediction is: \", predicted)\n",
        "\n",
        "print(\"Actual is is: \", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "XWWEdlWkCBtz",
        "outputId": "750fe325-8005-42df-d775-35b5d83ec5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Epoch 1/100, Tr Loss: 1.2261, Tr Acc: 60.5300, Val Loss: 0.3316, Val Acc: 89.8200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0212be5dbe5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# Put the network into training mode by each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Convert torch tensor to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMSArg0VClKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}